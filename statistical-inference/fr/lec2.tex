\chapter{Lecture 2}

\section{Cadre paramétrique}
\subsection{Modèle statistique paramétrique}
On dispose d'une observation ($X_1, \ldots, X_n$), un échantillon de variable
aléatoire i.i.d (indépendantes, identiquement distribuées) de loi commune $P$
appartenant à une famille de lois de probabilités paramétrée  $\{ P_{\theta,
\theta \in \Theta \subset \R^p} \}$.

\begin{remark}
   Si $\Theta \subset $ espace de dimension infinie $\rightarrow$ modèle non-paramétré.
\end{remark}
Estimer $P$ c'est estimer  $\theta \in \R^p$
\begin{eg}
    Bernolli($\theta$), Exponentielle($\theta$), $\mathcal{N}(\mu, \sigma^2)$, loi de densité  $f_{\theta}(x) = \theta x^{\theta - 1} 1_{x \in [0, 1]}$
\end{eg}
\begin{notation}
    $E_{\theta_n}[h(X_1, \ldots, X_n)]$, $Var_{\theta}[h(X_1, \ldots, X_n)]$

    Loi de $(X_1, \ldots, X_n)$ $\rightarrow$  $P_{\theta}^{\otimes n}$
\end{notation}

\begin{definition}[Estimateur]
    \[
    \hat{\theta} = \hat{\theta_n} = h(X_1, \ldots, X_n)
    \] 
\end{definition}
\begin{definition}[Qualité]
   \begin{itemize}
       \item Risque
           \[
               R(\hat{\theta}, \theta) = E_{\theta}[(\hat{\theta} - \theta)^2]
           \] 
        \item Consistance 
            \[
            \hat{\theta}_n \xrightarrow[n \to +\infty]{P} \theta
            \] 
   \end{itemize} 
\end{definition}
\begin{definition}[Modèle identifiable]
   \[
       \theta \to P_{\theta} \quad \text{injective}
   \]  
\end{definition}

\section{Méthode des moments}
\begin{definition}
    On appelle \textbf{moment théorique} de la loi de $X_i$ d'ordre $k$:  
    \[
        \mu_k = E[X_i^k], \quad k \ge 1
    \] 
\end{definition}
\begin{definition}
    On appelle  \textbf{moment empirique} de la loi des $X_i$ d'ordre $k$: 
     \[
    \hat{\mu}_k = \frac{1}{n}\sum_{i=1}^{n} X_i^k
    \] 
\end{definition}

Par la loi des grands nombres $\hat{\mu}_k \xrightarrow[n \to +\infty]{P} \mu_k$

% начало "что нахуй"
La méthode des moments: si on peut écrire $\theta$ ou  $g(\theta)$ paramètre d'intérêt comme une fonction des  $k$ premiers moments théoriques.
 \[
\theta = \mathcal{L}(\mu_1, \ldots, \mu_k)
\] 
alors l'estimateur 
\[
\hat{\theta} = \mathcal{L}(\hat{\mu_1}, \ldots, \mu_k)
\] 
est obtenu par la méthode.
% конец "что нахуй"

\begin{eg} Des calculs des estimateurs en utilisant la méthode des moments.
    \begin{itemize}
        \item $X_i \sim \operatorname{Bernoulli}(\theta)$ à valeurs 0-1,
            $\theta = P(X_i = 1) = E[X_i] \rightarrow \frac{1}{n}\sum_{i=1}^{n}
            X_i = \overline{X}$ 
        \item $X_i \sim \operatorname{Exp}(\theta), \, f_{\theta}(x) = \theta
            e^{-\theta x} 1_{x \ge 0}$, $E[X] = \frac{1}{\theta} \iff \theta =
            \frac{1}{\mu_1}$, par la méthode des moments, 
            \[
                \hat{\theta} = \frac{1}{\hat{\mu}_1} = \frac{1}{\overline{X}}
            \] 
            \begin{align*}
                Var_{\theta}(X_i) = \frac{1}{\theta^2} & \iff \theta^2 = \frac{1}{E[X_i^2] - E[X_i]^2}\\
                                                       & \iff \theta =
                                                       \frac{1}{\sqrt{\mu_2} -
                                                       \mu_1^2 } \implies
                                                       \hat{\theta}_2 =
                                                       \frac{1}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}
                                                       X_i^2 -
                                               (\overline{X})^2} }
            \end{align*}
        \item $X_1, \ldots, X_n$ i.i.d. de la loi $P_{\theta}$ de densité  
            \[
                f_{\theta}(x) = \theta x^{\theta - 1} 1_{x \in [0, 1]}
            \] 
            \begin{align*}
                E_{\theta}[X_i] = \theta \int_{{0}}^{{1}} x^{\theta} \: d{x} = \frac{\theta}{\theta + 1}
            \end{align*}
            Méthode des moments: 
            \begin{align*}
                        &(\theta + 1)\mu_1 = \theta \iff \theta(1 - \mu_1) = \mu_1 \iff \theta = \frac{E[X_i]}{1 - E[X_i]}\\
                \implies&\hat{\theta}_{MM} = \frac{\overline{X}}{1 - \overline{X}}, \, P_{\theta}(\overline{X} = 1) = P_{\theta}(X_1 = X_2 = \ldots = X_n = 1) = 0
            \end{align*}
    \end{itemize}
\end{eg}

\section{Rendu sur le L.A.C.}
(L.A.C = lemme des applications continues)
$(X_n)_{n \ge 1}$ suite de variables aléatoires. Si $X_n$ converge vers  $X$,
que peut-on dire de  $g(X_n)_{n \ge 1}$? Si $g$ continue, LAC.
 \begin{itemize}
    \item si $X_n \xrightarrow[]{P} X$ alors $g(X_n) \xrightarrow[]{P} g(X)$ 
    \item si $X_n \xrightarrow[]{\mathcal{L}} X$ alors $g(X_n) \xrightarrow[]{\mathcal{L}} g(X)$
\end{itemize}
\begin{remark}[Condition suffisante]
   \[
       D_g = \{ \text{points de discontinuité de } g \}
   \]  
   si $P(X \in D_g) = 0$, le LAC est vrai.
\end{remark}
\begin{eg}
    \begin{align*}
        g(x) =  \frac{x}{1 - x}
    \end{align*}
    \begin{itemize}
        \item LGN: $\overline{X} \xrightarrow[]{P} E[X]$ 
        \item LAC: $g(\overline{X}) = \hat{\theta}_n \xrightarrow[n \to +\infty]{P} g(E[X]) = \theta$
    \end{itemize}
\end{eg}

LAC pour des couples de suites de variables aléatoires:
\begin{itemize}
    \item si $(X_n, Y_n) \xrightarrow[]{P} (X, Y)$, alors $g(X_n, Y_n) \xrightarrow[]{P} g(X, Y)$, si $g: \R^2 \to  \R$ ou $\R^2$ continue
    \item si $(X_n, Y_n) \xrightarrow[]{\mathcal{L}} (X, Y)$, alors $g(X_n, Y_n) \xrightarrow[]{\mathcal{L}} g(X, Y)$
\end{itemize}

\begin{eg}
\[
    \hat{\theta}_2 = \frac{1}{\sqrt{\frac{1}{n}\sum_{i=1}^{n} X_i^2 - (\overline{X})^2} } \quad \text{consistant?}
\] 

LGN:
\begin{itemize}
    \item $\overline{X} \xrightarrow[]{P} \mu_1$
    \item $\frac{1}{n}\sum_{i=1}^{n} X_i^2 \xrightarrow[]{P} \mu_2$
\end{itemize}
donc 
\[
\begin{bmatrix} 
    \overline{X} \\ \frac{1}{n}\sum_{i=1}^{n} X_i^2
\end{bmatrix} 
\xrightarrow[]{P} \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix} 
\] 
$g(x, y) = \frac{1}{\sqrt{y - x^2} } \implies \hat{\theta}^{MM}$ constant de $\theta$,  $g$ continue sauf en $\{ (x, y) \in \R^2, y = x^2 \}$ de mesure nulle.

Mais c'est faux pour une converge en loi.
\end{eg}


\begin{prop}[Convergence de couples]
\[
\begin{pmatrix} X_n \\ Y_n \end{pmatrix} \xrightarrow[]{P} \begin{pmatrix} X \\ Y \end{pmatrix} \text{ ssi } \begin{cases}
    X_n \xrightarrow[]{P} X \\
    Y_n \xrightarrow[]{P} Y
\end{cases}
\] 
\end{prop}
\begin{newproof}
   \begin{itemize}
       \item $\implies$ alors LAC $g(x, y) = x$ continue donc  $X_n \to X$ et $Y_n \to  Y$ 
       \item $\impliedby$ convergence du couple?
           \[
               \forall \varepsilon > 0, P(|X_n - X| + |Y_n - Y| > \varepsilon) \le \underset{\to 0}{ P(|X_n - X| > \frac{\varepsilon}{2}) } + \underset{\to 0}{P(|Y_n - Y| > \frac{\varepsilon}{2})}
           \] 
           \underline{Cette réciproque est fausse pour la converge en loi!}
   \end{itemize} 
\end{newproof}
\subsection{Variance empirique}
Si la $X_i$ admettent une esperance  $\mu$ et une variance  $\sigma^2$, on appelle variance empirique
\begin{align*}
    \hat{\sigma}_n^2 &= \frac{1}{n}\sum_{i=1}^{n} (X_i - \overline{X})^2 = \frac{1}{n}\sum_{i=1}^{n} X_i^2 + \frac{1}{n}\sum_{i=1}^{n} \overline{X}^2 - \frac{2}{n}\sum_{i=1}^{n} X_i\overline{X} \\
                     &= \frac{1}{n}\sum_{i=1}^{n} X_i^2 + \overline{X}^2 - 2\overline{X}\overline{X} = \tilde{\sigma}^2
\end{align*}
estimateur des moments:
\[
    \sigma^2 = E[X_i^2] - E[X_i]^2
\] 
On remplace les moments théoriques par les moments impiriques
\[
    \rightarrow \tilde{\sigma^2}^{MM} = \frac{1}{n}\sum_{i=1}^{n} X_i^2 - (\overline{X})^2
\] 

Consistance:
$\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n} X_i^2 - (\overline{X})^2$, 
\[
    \begin{cases}
        \overline{X} \xrightarrow[]{P} E[X] \\
        \frac{1}{n}\sum_{i=1}^{n} X_i^2 \xrightarrow[]{P} E[X^2]
        \end{cases} \overset{\text{cv en proba}}{\implies} \begin{pmatrix} \overline{X} \\ \frac{1}{n}\sum_{i=1}^{n} X_i^2 \end{pmatrix} \overset{LAC}{\rightarrow} \hat{\sigma}^2 \text{ consistant de } Var(X) = E[X^2] - E[X]^2
\] 

\begin{ex}
    \begin{itemize}
        \item calculer le biais de $\hat{\sigma_n}^2$ 
        \item calculer le risque de $\hat{\sigma_n}^2$
    \end{itemize}
\end{ex}

\section{Méthode de maximum de vraisemblance}
\subsection{Modèle donné}
$(P_{\theta})_{\theta \in \Theta}$ est donné s'il existe une mesure $\mu$(positive $\sigma$ définie $\rightarrow$  $X_i$ à valeurs dans $E$,  $E = \cup E_n$ avec $\mu(E_n)$ finie) telle que $\forall \theta, P_{\theta}$ admet une densité par rapport à $\mu$.
\subsection{En pratique}
 \begin{itemize}
     \item soit $E$ au plus dénombrable:  $\mu$ = mesure de comptage. Si  $\exists \, \{ a_1, a_2, \ldots \}$ tq $\sum_{k \ge 1}^{} P_{\theta}(X_i = a_k) = 1$, alors $\mu = \sum_{k \ge 1}^{} \delta_{a_k}$ avec $\delta_a(\{a\}) = 1$ mesure de dirac.
         \begin{eg}
             $\operatorname{Bernoulli}(\theta)$, $X_i = 1$, probas  $\theta \to \mu = \delta_0 + \delta_1$ 
             On écrira 
             \[
                 f_{\theta}(x) = \overset{= 1 - \theta}{P_{\theta}(\{x\})} - P_{\theta}(X_i = x) \text{ avec } x \in \{a_1, a_2, \ldots \}
             \] 
         \end{eg}
     \item  soit $E = \R^p$, alors $f_{\theta}$ est la densité usuelle
\end{itemize}

$f_{\theta}$ densité de  $P_{\theta}$
\begin{definition}
    On appelle vraisemblance de l'échantillon  $(X_1, \ldots, X_n)$ la fonction
    \[
        \theta \rightarrow L_n(\theta) = \prod_{i=1}^{n} f_{\theta}(X_i) \text{ (variable aléatoire)}
    \] 
\end{definition}

\begin{definition}
    Un estimateur du max de vraisemblance $\hat{\theta}_{MV}$ est définie par:
    \[
    \forall \theta \in \Theta, L_n(\theta) \le L_n(\hat{\theta})
    \] 
\end{definition}

On travaille souvent avec la \textbf{log-vraisemblance} 
\[
    \log L_n(\theta) = \sum_{i=1}^{n} \ln f_{\theta}(X_i) \text{ somme de variables aléatoires}
\] 
\[
    \log L_n(\hat{\theta}) = \sup_{\theta \in \Theta} \log L_n(\theta)
\] 

\begin{remark}
    $\hat{\theta}$ est une variable aléatoire
\begin{figure}[H]
    \centering
    \incfig{remarque-max-vraisemblance-1}
    \caption{remarque-max-vraisemblance-1}
    \label{fig:remarque-max-vraisemblance-1}
\end{figure}
    
\end{remark}

\begin{eg}
   \begin{itemize}
       \item $\operatorname{Bernoulli}(\theta)$, $f_{\theta}(x) = \theta^x(1 - \theta)^{1-x}$,  $X_i$ à valeurs 0-1 

           \[
               L_n(\theta) = \prod_{i=1}^{n} \theta^{X_i}(1 - \theta)^{1 - X_i} = \theta^{\sum_{i=1}^{n} X_i}(1 - \theta)^{n - \sum_{i=1}^{n} X_i} 
           \] 
           \[
           \log L_n(\theta) = (\sum_{i=1}^{n} X_i) \ln \theta + (n - \sum_{i=1}^{n} X_i)\ln(1 - \theta)
           \] 
           \[
               (\log L_n)'(\theta) = \frac{\sum_{i=1}^{n} X_i}{\theta} - \frac{n - \sum_{i=1}^{n} X_i}{1 - \theta} = \frac{\sum_{i=1}^{n} X_i - n \theta}{\theta(1 - \theta)}(\overline{X} - \theta)
           \] 
           Equation de vraisemblance:
           \[
               (\log L_n)'(\theta) = 0 \iff (1 - \theta)\sum_{i=1}^{n} X_i = (n - \sum_{i=1}^{n} X_i)\theta) \iff \sum_{i=1}^{n} X_i = n\theta \implies \theta = \frac{\sum_{i=1}^{n} X_i}{n}
           \] 
           le point critique, est-il un maximum?
           \begin{itemize}
               \item La dérivée change de signe en $\overline{X}$  $\rightarrow$ on a bien un max  $\rightarrow$ $\hat{\theta}^{MV} = \overline{X}$
               \item Condition du 2nd ordre, si $(\log L_n)''(\theta) < 0$ pour tout $\theta$  $\implies$ $\log L_n$ est concave $\implies$ max global
           \end{itemize}
           \[
               (\log L_n)''(\theta) = - \frac{\sum_{i=1}^{n} X_i}{\theta^2} - \frac{n - \sum_{i=1}^{n} X_i}{(1 - \theta)^2} < 0, \, \forall \theta
           \] 
   \end{itemize} 
\end{eg}
