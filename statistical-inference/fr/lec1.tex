\chapter{Lecture 1}
\section*{Evaluation}
% 
\begin{itemize}
    \item $0.4$ CC $+ 0.6$ Examen.
    \item Répartition : $80\%$ partiel, $20\%$ Interro (prévue le 26/01).
\end{itemize}

\section{Modèle Statistique}

% 
\begin{definition}[Modèle Statistique]
    Un modèle statistique est un espace de probabilité $(\Omega, \mathcal{A}, \mathcal{P})$ où $\mathcal{P}$ est une famille de lois de probabilité $\{P_{\theta}; \theta \in \Theta\}$.
\end{definition}

% 
\begin{itemize}
    \item Si $\exists p \in \mathbb{N}^*, \Theta \subset \mathbb{R}^p$ : modèle paramétrique.
    \item Sinon : modèle non paramétrique.
\end{itemize}

% 
\begin{eg}[Familles de lois]
    \begin{itemize}
        \item Lois de Poisson : $\mathcal{P} = \{P(\lambda); \lambda > 0\}$.
        \item Densité régulière : $\mathcal{P} = \{\mathbb{P}; \mathbb{P} \text{ dont la densité admet une dérivée seconde bornée}\}$.
    \end{itemize}
\end{eg}

% 
\begin{definition}[Observation]
    Une observation est une variable aléatoire (v.a.) dont la loi appartient à $\{P_{\theta}, \theta \in \Theta\}$.
    Notre observation aura une structure de $n$-échantillons $X_1, \dots, X_n$ i.i.d. (indépendants et identiquement distribués) de loi commune $\in \{P_{\theta}, \theta \in \Theta\}$.
\end{definition}

% 
\begin{remark}
    $(X_1, \dots, X_n)$ est de loi $P_{\theta}^{\otimes n}$. L'échantillon contient toute l'information sur $P_{\theta}$, donc sur $\theta$.
\end{remark}

% 
\begin{definition}[Identifiabilité]
    Un modèle est identifiable si et seulement si (ssi) l'application $\theta \mapsto P_{\theta}$ est injective.
\end{definition}

\section{Estimateurs}

% 
\textbf{Hypothèse :} On observe $X_1, \dots, X_n$ i.i.d. de loi commune $\in \{P_{\theta}, \theta \in \Theta \subset \mathbb{R}^p\}$ (modèle paramétrique identifiable). Soit $\theta^*$ la vraie valeur inconnue telle que $P_{X_i} = P_{\theta^*}$.

% 
\begin{definition}[Estimateur]
    Un estimateur de $\theta$ est une fonction de l'échantillon $(X_1, \dots, X_n)$ mesurable et indépendante de $\theta$ (calculable à partir des données).
\end{definition}

% 
\textbf{Notation :} $\hat{\theta} = \hat{\theta}_n = h(X_1, \dots, X_n)$. C'est une variable aléatoire. \\
Exemples : $\hat{\theta} = \bar{X}$, $\hat{\theta} = X_1 - X_3$, etc.

Questions fondamentales :
\begin{enumerate}
    \item Comment définir un bon estimateur ?
    \item Comment construire un bon estimateur ?
\end{enumerate}

\section{Risque quadratique}

% 
\textbf{Idée :} En moyenne, $\hat{\theta}$ doit être proche de $\theta$. On regarde $\mathbb{E}[\hat{\theta} - \theta]$.

% 
\begin{definition}[Biais]
    Le biais de $\hat{\theta}$ est défini par :
    $$ B(\hat{\theta}, \theta) = \mathbb{E}[\hat{\theta}] - \theta $$
    On dit que $\hat{\theta}$ est \textbf{sans biais} si $B(\hat{\theta}, \theta) = 0$.
\end{definition}

% 
\begin{definition}[Risque quadratique / MSE]
    $$ R(\hat{\theta}, \theta) = \mathbb{E}[(\hat{\theta} - \theta)^2] $$
    C'est la Mean Squared Error (MSE).
\end{definition}

% 
On dit que $\hat{\theta}_1$ est meilleur que $\hat{\theta}_2$ ssi $R(\hat{\theta}_1, \theta) \le R(\hat{\theta}_2, \theta)$.

\subsection*{Exemple : Modèle de Poisson}
% 
Soit $X_1, \dots, X_n$ de loi $P_{\theta}$ de Poisson, $\theta > 0$. On cherche un estimateur de $\theta = \mathbb{E}[X_i]$.

% 
Proposons : $\hat{\theta} = \overline{X} = \frac{1}{n}\sum_{i=1}^{n}X_i$.

\textbf{Calcul du Biais :} % 
\begin{align*}
    B(\hat{\theta}, \theta) &= \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}X_i\right] - \theta \\
    &= \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}[X_i] - \theta \quad (\text{par linéarité}) \\
    &= \frac{1}{n} \cdot n \cdot \mathbb{E}[X_1] - \theta \\
    &= \theta - \theta = 0
\end{align*}
Donc $\mathbb{E}[\overline{X}] = \theta$, l'estimateur est sans biais. % 

\textbf{Calcul du Risque :} % 
\begin{align*}
    R(\hat{\theta}, \theta) &= \mathbb{E}[(\overline{X} - \theta)^2] = \mathbb{E}[(\overline{X} - \mathbb{E}[\overline{X}])^2] \\
    &= Var(\overline{X}) = Var\left(\frac{1}{n}\sum X_i\right) \\
    &= \frac{1}{n^2} \sum Var(X_i) \quad (\text{car i.i.d}) \\
    &= \frac{1}{n^2} \cdot n \cdot Var(X_1) = \frac{Var(X_1)}{n} = \frac{\theta}{n}
\end{align*}

% 
\begin{prop}[Décomposition Biais-Variance du risque]
    $$ R(\hat{\theta}, \theta) = (B(\hat{\theta}, \theta))^2 + Var(\hat{\theta}) $$
\end{prop}

% 
\begin{newproof}
    \begin{align*}
        R(\hat{\theta}, \theta) &= \mathbb{E}[(\hat{\theta} - \theta)^2] \\
        &= \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}] + \mathbb{E}[\hat{\theta}] - \theta)^2] \\
        &= \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2] + \mathbb{E}[(\mathbb{E}[\hat{\theta}] - \theta)^2] + 2\mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])(\mathbb{E}[\hat{\theta}] - \theta)] \\
        &= Var(\hat{\theta}) + (B(\hat{\theta}, \theta))^2 + 2(\mathbb{E}[\hat{\theta}] - \theta) \underbrace{\mathbb{E}[\hat{\theta} - \mathbb{E}[\hat{\theta}]]}_{0} \\
        &= Var(\hat{\theta}) + B(\hat{\theta}, \theta)^2
    \end{align*}
\end{newproof}

\section{Consistance}
% 
Propriété asymptotique. On ne considère que des estimateurs consistants.

% 
\begin{definition}[Consistance]
    Soit $(X_1, \dots, X_n)$ i.i.d. de loi $P_{\theta}$. Soit $\hat{\theta}_n = h(X_1, \dots, X_n)$.
    $\hat{\theta}_n$ est un estimateur consistant (ou convergent) de $\theta$ ssi :
    $$ \hat{\theta}_n \xrightarrow[n \to +\infty]{\mathbb{P}} \theta $$
\end{definition}

% 
\begin{remark}
    $\hat{\theta}_n$ est fortement consistant ssi $\hat{\theta}_n \xrightarrow[n \to +\infty]{p.s.} \theta$.
\end{remark}

\subsection*{Exemple : Retour au modèle de Poisson}
% 
$\Theta = \mathbb{R}_+^*$, $\hat{\theta}_n = \overline{X}$.

% 
\begin{itemize}
    \item On peut invoquer la Loi des Grands Nombres (LGN) : $\overline{X} \xrightarrow{\mathbb{P}} \mathbb{E}[X_i] = \theta$.
    \item Via le risque quadratique :
    % 
    $$ R(\hat{\theta}_n, \theta) = Var(\overline{X}) = \frac{\theta}{n} \xrightarrow{n \to +\infty} 0 $$
    D'après l'inégalité de Bienaymé-Tchebychev :
    $$ P(|\hat{\theta}_n - \theta| > \epsilon) \le \frac{\mathbb{E}[(\hat{\theta}_n - \theta)^2]}{\epsilon^2} = \frac{R(\hat{\theta}_n, \theta)}{\epsilon^2} \to 0 $$
\end{itemize}

\subsection*{Méthode "Plug-in"}
% 
Soit $(X_1, \dots, X_n)$ i.i.d. Poisson$(\theta)$. On veut estimer $\beta = P(X_i = 0) = e^{-\theta}$.
$$ \hat{\beta} = e^{-\hat{\theta}} = e^{-\overline{X}} $$

% 
$\hat{\beta}$ est consistant pour estimer $\beta$.

% 
\begin{lemma}[Lemme de l'application continue]
    Si $Z_n \xrightarrow{\mathbb{P}} Z$, alors $h(Z_n) \xrightarrow{\mathbb{P}} h(Z)$ pour toute fonction continue $h$.
\end{lemma}
